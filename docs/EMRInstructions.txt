1. When you setup the EMR cluster, set the bootstrap action to custom action,
and specify the location as
s3://support.elasticmapreduce/spark/install-spark

2. Once the cluster is ready, SSH into the master node:
ssh -i <keypair location> hadoop@<master-dns-address>
Create two folders in the master  - data and code 
mkdir data
mkdir code

3. Add a user to master node
sudo useradd rhea
sudo passwd rhea

4. Install PostgreSQL on master node
sudo yum install postgresql postgresql-server
sudo service postgresql initdb

5. Configure PostgreSQL on master node
i. Change authentication models for local to trust.
ii. Change authentication for any host i.e. 0.0.0.0/0 to md5
iii. Change port and listen_address

sudo su postgres 
cd /var/lib/pgsql9/data
vim pg_hba.conf 
local   all         all                                  trust
host    all         all         0.0.0.0/0          md5

vim postgresql.conf
listen_addresses = '*' 
port = 5432

exit

6. Start PostgreSQL
sudo service postgresql start -p 5432

7. Transfer data files from local machine to remote master node

scp -i <keypair> data/exactnew.sql hadoop@<master-dns-address>:/home/hadoop/data

scp -i <keypair> data/vocab.sql hadoop@<master-dns-address>:/home/hadoop/data

8. Create databases and load data on master node

sudo su postgres
createdb vocab
createdb exact
psql -d vocab

vocab=# CREATE USER rhea with password 'abcde';
CREATE ROLE
vocab=# GRANT ALL PRIVILEGES ON DATABASE vocab to rhea;
GRANT
\q

psql -d exact
GRANT ALL PRIVILEGES ON DATABASE exact to rhea;
\q

exit

psql -d exact -f data/exactnew.sql -U rhea
psql -d vocab -f data/vocab.sql -U rhea


9. Transfer source code from local machine to remote master node
scp -i <keypair> -r src hadoop@<master-dns-address>:/home/hadoop/code
scp -i <keypair> -r sbt hadoop@<master-dns-address>:/home/hadoop/code
scp -i <keypair> build.sbt hadoop@<master-dns-address>:/home/hadoop/code
scp -i <keypair> project/build.properties hadoop@<master-dns-address>:/home/hadoop/code/project
scp -i <keypair> project/plugins.sbt hadoop@<master-dns-address>:/home/hadoop/code/project
scp -i <keypair> dependency/postgresql-9.3-1103.jdbc41.jar hadoop@<master-dns-address>:/home/hadoop/code

10. Change host in application.conf in resource to use master's ip address
vim src/main/resources/application.conf
Change host from localhost, to ip-address of master node 

11. Create the uber jar
> sbt compile assembly

12. Run the code
./spark-submit --class edu.gatech.cse8803.main.Main --master yarn-client --driver-memory 10G --executor-memory 10G --executor-cores 2 --num-executors 4 --jars /home/hadoop/code/postgresql-9.3-1103.jdbc41.jar /home/hadoop/code/target/scala-2.10/cse8803_project_template-assembly-1.0.jar
